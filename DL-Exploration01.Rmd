---
title: "David's initial exploration"
author: "David Lovell"
date: "`r Sys.Date()`"
output: html_document
---


```{r setup, include=FALSE}
# Set options to control the behaviour of knitr, dplyr, etc
knitr::opts_chunk$set(echo = FALSE, comment=NA, warning = FALSE, message = FALSE)
options(knitr.table.format = "html", dplyr.summarise.inform = FALSE, tidyverse.quiet = TRUE, width = 100)
```

```{r load-libraries, warning=FALSE, message=FALSE, include=FALSE}
rm(list = ls())
library(tidyverse)
library(scales)    # For trans_format()
library(patchwork)
```

# Try reading in one of Khoa's files

Khoa has sent us a range of `.csv` files containing the predictions of the proportion of different cell types in a sample.
He's sent us the predictions made by 8 different models, and he's sent us the ground truth.

Our first job is to ensure we can ingest and interpret these data files.

Let's try reading `M1.csv`:

```{r}
read_tsv("./data/M1.csv")
```

Cool! That works. It is actually a TAB separated value file, not a `.csv` and the first column has no name (the string `...1` is used as a legal placeholder) so we can fix that with...

```{r}
read_tsv("./data/M1.csv") %>% rename(ID="...1")
```

# Try reading in the truth

So, let's see what we have here:
```{r}
read_tsv("./data/truth.csv")
```

OK! So we have a similar file of IDs and (I hope) proportions.

# Combine model predictions with ground truth

I think a good way to do this is to do a database join using the ID as a key... I'm not quite sure how to do this, so I will have a play with some cut down data:
```{r}
read_tsv("./data/M1.csv", n_max=10, show_col_types = FALSE) %>% rename(ID="...1") %>% select(ID, A, B) -> method1
read_tsv("./data/truth.csv",    n_max=10, show_col_types = FALSE) %>% rename(ID="...1") %>% select(ID, A, B) -> truth
```

Let's do a _full_ join (I think!) using the suffixes `.pred` for "predicted" and `.act` for "actual"

```{r}
full_join(method1, truth, by="ID", suffix=c(".pred", ".act"))
```

Looking good! Now let's see how we might ingest all 8 models' worth of predictions. If the data is small enough, my usual strategy is to create a single data frame that I can then subset. If the data is too big for that, I'm forced to work in chunks.

# Let's try reading in all the model predictions

There is probably a neater more compact way to do this, but I'm happy to be verbose.

```{r}
n_max <- 10
bind_rows(
  .id="model",
  read_tsv("./data/M1.csv", n_max=n_max, show_col_types = F),
  read_tsv("./data/M2.csv", n_max=n_max, show_col_types = F)
) %>% 
  rename(ID="...1") -> predictions
```

Now get the truth:
```{r}
read_tsv("./data/truth.csv",    n_max=n_max, show_col_types = F) %>% rename(ID="...1")  -> truth
```

Now join to the truth:
```{r}
full_join(predictions, truth, by="ID", suffix=c(".pred", ".act")) -> khoa
```

# Show how to convert to long format

`ggplot()` requires data in _long_ format, i.e., one measurement per row. What I want to get to is data in the form of

```
model ID         component predicted actual
1     96b0a24bâ€¦  A         0         0.0591
...
```

This is a job for `pivot_longer()`. I'm going to approach this in two stages maybe...
```{r}
pivot_longer(
  khoa, 
  cols= -c(model, ID), 
  names_to = c(".value", "label"), 
  names_pattern = "(.).(.*)")
```

Close but no cigar! After looking at `vignette("pivot")` 


```{r}
khoa %>% 
  pivot_longer(
    cols= -c(model, ID), 
    names_to = c(".value", "label"), 
    names_pattern = "(.).(.*)") %>%
  pivot_longer(cols=-c(model, ID, label), names_to = "component") %>%
  pivot_wider(names_from = label) -> khoa.long
```


```{r}
ggplot(khoa.long, aes(x=act, y=pred, group=fct_cross(model, ID))) +
  geom_point() + 
  geom_line() + 
  coord_equal(xlim=c(0,1), ylim=c(0,1)) + 
  facet_grid(model ~ component)
```



```{r}
ggplot(khoa.long, aes(x=act, y=pred)) +
  geom_abline(slope=1, color="white") + 
  geom_point() + 
  coord_equal(xlim=c(0,1), ylim=c(0,1)) + 
  facet_grid(model ~ component)
```


# Now ingest the full dataset


```{r}
n_max <- Inf
bind_rows(
  .id="model",
  read_tsv("./data/M1.csv", n_max=n_max, show_col_types = F),
  read_tsv("./data/M2.csv", n_max=n_max, show_col_types = F),
  read_tsv("./data/M3.csv", n_max=n_max, show_col_types = F),
  read_tsv("./data/M4.csv", n_max=n_max, show_col_types = F),
  read_tsv("./data/M5.csv", n_max=n_max, show_col_types = F),
  read_tsv("./data/M6.csv", n_max=n_max, show_col_types = F),
  read_tsv("./data/M7.csv", n_max=n_max, show_col_types = F),
  read_tsv("./data/M8.csv", n_max=n_max, show_col_types = F)
) %>% 
  rename(ID="...1") -> predictions
```

Now get the truth:
```{r}
read_tsv("./data/truth.csv",    n_max=n_max, show_col_types = F) %>% rename(ID="...1")  -> truth
```

Now join to the truth:
```{r}
full_join(predictions, truth, by="ID", suffix=c(".predicted", ".actual")) -> khoa
```

```{r}
khoa %>% 
  pivot_longer(
    cols= -c(model, ID), 
    names_to = c(".value", "label"), 
    names_pattern = "(.).(.*)") %>%
  pivot_longer(cols=-c(model, ID, label), names_to = "component") %>%
  pivot_wider(names_from = label) -> khoa.long
```

```{r}
saveRDS(khoa, "./data/khoa.RDS")
saveRDS(khoa.long, "./data/khoa.long.RDS")
```

# Visualise data

The fun bit! But first, let's make sure we understand the distribution of the actual and predicted component values. From what I know, the actuals will take on a finite set of values
```{r}
length(unique(khoa.long$actual))
```

Here's their distribution:
```{r}
khoa.long %>%
  pivot_longer(cols=-c(model, ID, component), names_to = "label") %>%
  ggplot(aes(x=value)) + 
  geom_histogram(binwidth = 1e-3) +
  facet_grid(label~.)
```

ooh. Lots of zeros and then actual cases at 0.05 intervals. Hmmm, the zeros will be a challenge for log transformations. Let's just filter them out to get a sense of the number of non-zero component values.

Here's their distribution:
```{r}
khoa.long %>%
  pivot_longer(cols=-c(model, ID, component), names_to = "label") %>%
  filter(value > 0) %>%
  ggplot(aes(x=value)) + 
  geom_histogram(binwidth = 1e-3) +
  facet_grid(label~.)
```

Wow, still a lot of very small predictions... let's zoom in a bit:
```{r}
khoa.long %>%
  pivot_longer(cols=-c(model, ID, component), names_to = "label") %>%
  filter(0 < value, value < 1e-3) %>%
  ggplot(aes(x=value)) + 
  geom_histogram(binwidth = 1e-6) +
  facet_grid(label~.)
```

Goodness! Let's zoom in even more!
```{r}
khoa.long %>%
  pivot_longer(cols=-c(model, ID, component), names_to = "label") %>%
  filter(0 < value, value < 1e-10) %>%
  ggplot(aes(x=value)) + 
  geom_histogram(binwidth = 1e-13) +
  facet_grid(label~.)
```
That's pretty crazy huh? We have models that are estimating the abundance of cells in a sample to be 1 in a trillion. Given that there are estimated to be about 30 trillion (human) cells in the human body, I doubt we could get a trillion cells in a needle biopsy, unless we had a really big needle.

The distribution of actual proportions Makes sense because every time we set one of the components to be large, there will be lots of smaller components to fit around. I wonder what the smallest non-zero actual and predicted values are?

```{r}
khoa.long %>% filter(actual    > 0) %>% pull(actual) %>% min
khoa.long %>% filter(predicted > 0) %>% pull(predicted) %>% min
```

```{r}
lims <- c(1e-3,1e0)
npoints <- 10000
```

Here are `r npoints`  out of `r nrow(khoa.long)` (i.e., `r round(npoints * 100 / nrow(khoa.long),3)`%) of the data:

```{r}
ggplot(slice_sample(khoa.long, n=npoints), aes(x=predicted, y=actual)) +
  geom_abline(slope=1, color="white") + 
  geom_point(alpha=0.1) + 
  scale_x_log10() + scale_y_log10() +
  coord_equal(xlim=lims, ylim=lims) + 
  theme(axis.text.x = element_text(angle = 90)) +
  facet_grid(model ~ component, labeller=label_both) + 
  labs(
    title="Actual component proportions vs predicted proportions",
    subtitle=sprintf("Showing a sample of %d points on a log-log scale from %0.1e to %0.1e", npoints, lims[1], lims[2])
  )
```

Model 3 looks pretty crap!

For the full dataset, we don't want to be plotting a bajillion points, so let's look at the density contours:
```{r}
ggplot(slice_sample(khoa.long, n=npoints), aes(x=predicted, y=actual)) +
  geom_abline(slope=1, color="white") + 
  geom_density_2d() + 
  scale_x_log10() + scale_y_log10() +
  coord_equal(xlim=lims, ylim=lims) + 
  theme(axis.text.x = element_text(angle = 90)) +
  facet_grid(model ~ component, labeller=label_both) + 
  labs(
    title="Actual component proportions vs predicted proportions",
    subtitle=sprintf("The density of a sample of %d points on a log-log scale from %0.1e to %0.1e", npoints, lims[1], lims[2])
  )
```

Hmmm... that's not great. Maybe the data is too dense at points off the plot? Let's filter out the zero actuals and predictions and see

```{r}
khoa.long %>% filter(actual > 0, predicted > 0) %>% slice_sample(n=npoints) %>% 
ggplot(aes(x=predicted, y=actual)) +
  geom_abline(slope=1, color="white") + 
  geom_density_2d() + 
  scale_x_log10() + scale_y_log10() +
  coord_equal(xlim=lims, ylim=lims) + 
  theme(axis.text.x = element_text(angle = 90)) +
  facet_grid(model ~ component, labeller=label_both) + 
  labs(
    title="Predicted component proportions vs actual proportions",
    subtitle=sprintf("The density of a sample of %d points on a log-log scale from %0.1e to %0.1e", npoints, lims[1], lims[2]),
    caption="Points with zero actual or predicted values have been removed."
  )
```

Ah, now I think I get it: the contours apply to all the facets, and the density of points in some of the facets never reaches the lowest contour. Let's try putting the breaks on a log10 scale

```{r}
khoa.long %>% filter(actual > 0, predicted > 0) %>% slice_sample(n=npoints) %>% 
ggplot(aes(x=predicted, y=actual)) +
  geom_abline(slope=1, color="white") + 
  geom_density_2d(breaks=10^(-(9:1))) + 
  scale_x_log10() + scale_y_log10() +
  coord_equal(xlim=lims, ylim=lims) + 
  theme(axis.text.x = element_text(angle = 90)) +
  facet_grid(model ~ component, labeller=label_both) + 
  labs(
    title="Predicted component proportions vs actual proportions",
    subtitle=sprintf("The density of a sample of %d points on a log-log scale from %0.1e to %0.1e", npoints, lims[1], lims[2]),
    caption="Points with zero actual or predicted values have been removed."
  )
```

Yuk... lumpy contours... let's log the data _then_ plot the contours:
```{r eval=FALSE}
khoa.long %>% filter(actual > 0, predicted > 0) %>% slice_sample(n=npoints) %>% 
ggplot(aes(x=log10(predicted), y=log10(actual))) +
  geom_abline(slope=1, color="white") + 
  geom_density_2d(breaks=10^(-(9:1))) + 
  # cale_x_log10() + scale_y_log10() +
  coord_equal(xlim=log10(lims), ylim=log10(lims)) + 
  theme(axis.text.x = element_text(angle = 90)) +
  facet_grid(model ~ component, labeller=label_both) + 
  labs(
    title="Predicted component proportions vs actual proportions",
    subtitle=sprintf("The density of a sample of %d points on a log-log scale from %0.1e to %0.1e", npoints, lims[1], lims[2]),
    caption="Points with zero actual or predicted values have been removed."
  )
```

Meh!

OK, another approach. Rather than visualise predicted versus actual, let's look at the _relative difference_ between predicted and actual, vs actual. (NB. this takes a few minutes for 1M points... but it's worth it)

```{r}
npoints <- 10000
xlim <- c(1e-9,1e0)
ylim <- c(1e-4, 1e4)

clip <- function(x, low, high){
  pmin(high, pmax(low, x)) 
}

khoa.long %>% filter(actual > 0, predicted > 0) %>% slice_sample(n=npoints) %>% 
ggplot(
  aes(
    x=clip(predicted, xlim[1], xlim[2]),
    y=clip(predicted/actual, ylim[1], ylim[2]))
  ) +
  geom_hline(yintercept=1,  lty=2) + 
  geom_point(alpha=0.25, size=0.5) + 
  geom_smooth(method = "loess") + 
  scale_x_log10(labels = trans_format("log10", math_format(10^.x))) +
  scale_y_log10(labels = trans_format("log10", math_format(10^.x)), breaks=10^seq(from=-4, to=4, by=2)) +
  coord_equal(xlim=xlim, ylim=ylim) + 
  theme(axis.text.x = element_text(angle = 90)) +
  facet_grid(model ~ component, labeller=label_both) + 
  labs(
    x="predicted", y="predicted/actual",
    title="Predicted/actual component proportions vs predicted proportions",
    subtitle=sprintf("Showing a sample of %d points on a log-log scale from x=%0.1e to x=%0.1e", npoints, lims[1], lims[2]),
    caption="Points with zero actual or predicted values have been removed. Values outside the range of an axes have been set to the nearest in range value."
  )
```


The big'un. I'm setting `eval=FALSE` so I can knit this document in a reasonable time.

```{r eval=FALSE}
xlim <- c(1e-9,1e0)
ylim <- c(1e-4, 1e4)

clip <- function(x, low, high){
  pmin(high, pmax(low, x)) 
}

khoa.long %>% filter(actual > 0, predicted > 0) %>% 
ggplot(
  aes(
    x=clip(predicted, xlim[1], xlim[2]),
    y=clip(predicted/actual, ylim[1], ylim[2]))
  ) +
  geom_hline(yintercept=1,  lty=2) + 
  geom_point(alpha=0.25, size=0.1) + 
  # geom_smooth(method = "loess") + 
  scale_x_log10(labels = trans_format("log10", math_format(10^.x))) +
  scale_y_log10(labels = trans_format("log10", math_format(10^.x)), breaks=10^seq(from=-4, to=4, by=2)) +
  coord_equal(xlim=xlim, ylim=ylim) + 
  theme(axis.text.x = element_text(angle = 90)) +
  facet_grid(model ~ component, labeller=label_both) + 
  labs(
    x="predicted", y="predicted/actual",
    title="Predicted/actual component proportions vs predicted proportions",
    subtitle=sprintf("Showing a sample of %d points on a log-log scale from x=%0.1e to x=%0.1e", npoints, lims[1], lims[2]),
    caption="Points with zero actual or predicted values have been removed. Values outside the range of an axes have been set to the nearest in range value."
  )
```


```{r eval=FALSE}
ggsave("plots/relative-vs-actual.png", width = 10, height=10, dpi=600)
```

